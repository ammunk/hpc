#!/usr/bin/bash

# For SLURM see: https://docs.computecanada.ca/wiki/Using_GPUs_with_Slurm and https://docs.computecanada.ca/wiki/Running_jobs
# For PBS see: https://hpcc.usc.edu/support/documentation/running-a-job-on-the-hpcc-cluster-using-pbs/
# Comparing commands see: https://hpcc.usc.edu/support/documentation/pbs-to-slurm/

# THIS SCRIPT ALLOWS ONLY THE USE OF A SINGLE NODE!

# For SLURM: each submission is considered a SINGLE task (UNLESS WE USE MPI)

# IF USING MPI WITH SEVERAL "TASKS" - I.E. USING SEVERAL NODES: USE DIFFERENT SCRIPT

# determine cluster used
if [[ "$(hostname)" == *"borg"* ]]; then
    CLUSTER="PLAI"
elif [[ "$(hostname)" == *"cedar"* ]]; then
    CLUSTER="CC"
fi

# general settings
PROJECT_DIR="PROJECT_NAME" # project name
EXP_NAME="EXP_NAME" # experiment name used by e.g. sacred
CPUS_PER_TASK=6 # should be no more than 6 per GPU! -- SEE COMPUTE CANADA
MEM="30G" # G/M is Gigabyte/Megabyte (see cedar on compute canada for amount of memory we can get per core)
TIME="23:59:00" # <HH:MM:SS>
GPUS=1
# PLAI specific arguments - partition (plai/plai_towers/plai_cpus)
PARTITION="plai"

# an overlay where the container need write permissions
# (MAKE SURE IT CORRESPONDS WITH WHATEVER IS DEFINED IN SINGULARITY FILE)
OVERLAYDIR_CONTAINER="/overlay"

# container arguments
CONTAINER_NAME="container.sif"

# command send to container
CMD_BASE=""

function array_to_pearl () {
    echo "($(echo "$1" | sed 's/ /, /g'))"
}

function geometric {
    ((val = $1))
    ((mult = $2))
    ((count = $3))

    tmp="$val"
    while [[ ${count} -gt 0 ]] ; do
        ((val *= mult))
        ((count -= 1))
        tmp="${tmp}, ${val}"
    done
    echo $tmp
}

# specify options over which to sweep (for instance batch_size)
example1=$(geometric 1 2 10)
example2="($(seq -s ', ' 1 20))"
example3="($(seq -s ', ' 25 25 1000))"

# EXTEND FOR LOOP WITH MORE OPTIONS
OPTIONS="$(perl -le '@ex1 = '"$(array_to_pearl "${example1}")"';
                        @ex2 ='"$(array_to_pearl "${example2}")"';
                        @ex3 ='"$(array_to_pearl "${example3}")"';
                        for my $i (0 .. $#hd){
                            print "ex1=$ex1[int($i/4)] ".
                            "ex2=$ex2[int($i/2)] ".
                            "ex3=$ex3[int($i)]"
                            }')"


# create array which splits string on \n from above
# readarray -t OPTIONS <<< $OPTIONS # ONLY BASH 4.4+
IFS=$'\n' read -rd '' -a OPTIONS <<<"$OPTIONS"
N=${#OPTIONS[@]}

# clear array_command_list.txt before putting thins into it
array_commands="array_command_list_${EXP_NAME}.txt"
> "${array_commands}"
for option in "${OPTIONS[@]}"; do
    echo "${CMD_BASE} ${option}" >> "${array_commands}"
done

# Directory where this script is called
DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"

EXP_DIR="$(dirname "$DIR")" # get parent of parent directory from where this script is called (assumes submitted from exp_dir/hpc_scripts/some_dir)
RESULTS_MOUNT="/code/results" # where results is located in the siongularity container

# LEAVE EMPTY SO THAT WE DO NOT COPY ANYTHING TO SLURM_TMPDIR
STUFF_TO_TAR=""

# set base directory names to store results from jobs (specific directory will be given in the job script)
if [ ! -z ${SCRATCH} ]; then

    BASERESULTSDIR="${SCRATCH}/${PROJECT_DIR}"

    # create hpc_output directory if it does not exist
    if [ ! -d "${BASERESULTSDIR}/${EXP_NAME}/hpc_output" ]; then
        mkdir -p "${BASERESULTSDIR}/${EXP_NAME}/hpc_output"
    fi

    if [ ! -d "${BASERESULTSDIR}/${EXP_NAME}/results" ]; then
        mkdir -p "${BASERESULTSDIR}/${EXP_NAME}/results"
    fi

    VARIABLES="CODE_DIR=${EXP_DIR},OVERLAYDIR_CONTAINER=${OVERLAYDIR_CONTAINER},\
CONTAINER=${CONTAINER_NAME},RESULTS_MOUNT=${RESULTS_MOUNT},\
STUFF_TO_TAR=${STUFF_TO_TAR},BASERESULTSDIR=${BASERESULTSDIR}"

    # SUBMIT JOBS
    CMD_N="${CMD_BASE_N}"

    sbatch_cmd="--job-name=${EXP_NAME} \
                --time=${TIME} \
                --cpus-per-task=${CPUS_PER_TASK} \
                --ntasks-per-core=1 \
                --ntasks=1 \
                --mem=${MEM} \
                --chdir=${BASERESULTSDIR} \
                -o ${BASERESULTSDIR}/${EXP_NAME}/hpc_output/${EXP_NAME}_%A_%a.out \
                --export=ALL,\"${VARIABLES}\",EXP_NAME=${EXP_NAME}"

    re='^[0-9]+$'
    if  [[ ! $GPUS =~ $re ]] ; then
        echo "error: GPUS must be interger" >&2; exit 1
    else
        if [[ "$CLUSTER" = "PLAI" ]]; then
            if [[ "$GPUS" != "0" ]]; then
                sbatch_cmd="${sbatch_cmd} --gres=gpu:${GPUS}"
            fi
            sbatch_cmd="${sbatch_cmd} --array=1-${N}%10 --partition=${PARTITION}"
            bash -c "sbatch ${sbatch_cmd[@]} ~/hpc/plai_array_job.sh"
        elif [[ "$CLUSTER" = "CC" ]]; then
            if [[ "$GPUS" != "0" ]]; then
                sbatch_cmd="${sbatch_cmd} --gres=gpu:v100l${GPUS}"
                #sbatch_cmd="${sbatch_cmd} --gres=gpu:${GPUS}"
            fi
            sbatch_cmd="${sbatch_cmd} --array=1-${N}"
            bash -c "sbatch ${sbatch_cmd[@]} ~/hpc/cc_array_job.sh"
        fi
        sleep 1 # sleep so that we do not overwhelm the scheduler (SEE e.g. compute canada's website)
    fi
else
    echo "error while submitting job" >&2; exit 1
fi
