#!/usr/bin/bash

# For SLURM see: https://docs.computecanada.ca/wiki/Using_GPUs_with_Slurm and https://docs.computecanada.ca/wiki/Running_jobs
# For PBS see: https://hpcc.usc.edu/support/documentation/running-a-job-on-the-hpcc-cluster-using-pbs/
# Comparing commands see: https://hpcc.usc.edu/support/documentation/pbs-to-slurm/

# THIS SCRIPT ALLOWS ONLY THE USE OF A SINGLE NODE!

# For SLURM: each submission is considered a SINGLE task (UNLESS WE USE MPI)

# IF USING MPI WITH SEVERAL "TASKS" - I.E. USING SEVERAL NODES: USE DIFFERENT SCRIPT

# determine cluster used
if [[ "$(hostname)" == *"borg"* ]]; then
    CLUSTER="PLAI"
elif [[ "$(hostname)" == *"cedar"* ]] || [[ "$(hostname)" == *"gra"* ]]; then
    CLUSTER="CC"
fi

# general settings
PROJECT_DIR="PN" # project name
EXP_NAME="TEST"
CPUS_PER_TASK=1 # should be no more than 6 per GPU! -- SEE COMPUTE CANADA
ACCOUNT='rrg-kevinlb' # def-fwood or rrg-kevinlb
MEM="5G" # G/M is Gigabyte/Megabyte
TIME="01:00:00" # <HH:MM:MM>
GPUS=1 # set to 0 to not ask for any
# PLAI specific arguments - partition (plai/plai_towers/plai_cpus)
PARTITION="plai"

# an overlay where the container need write permissions
# (MAKE SURE IT CORRESPONDS WITH WHATEVER IS DEFINED IN SINGULARITY FILE)
WORKDIR_MOUNT="/workdir"
RESULTS_MOUNT="/results" # where results is located in the singularity container
# container arguments
CONTAINER_NAME="container.sif"
# LEAVE EMPTY SO THAT WE DO NOT COPY ANYTHING TO SLURM_TMPDIR
STUFF_TO_TMP=""
# LEAVE EMPTY SO THAT WE DO COPY ENTIRE RESULTS TO BASEDIR
RESULTS_TO_SCRATCH=""

# command send to container
CMD_BASE=""

# sweeping
DOSWEEP=true

if [[ $DOSWEEP = true ]]; then
    function array_to_pearl () {
        echo "($(echo "$1" | sed 's/ /, /g'))"
    }

    CMDs=()
    # specify options over which to sweep (for instance batch_size)
    batch_size="64"

    # EXTEND FOR LOOP WITH MORE OPTIONS
    OPTIONS="$(perl -le '@batch_size = '"$(array_to_pearl "${batch_size}")"';
                     for $b (@batch_size){
                         print "batch_size=$b"}')"
    # create array which splits string on \n from above
    # readarray -t OPTIONS <<< $OPTIONS # ONLY BASH 4.4+
    IFS=$'\n' read -rd '' -a OPTIONS <<<"$OPTIONS"

    for option in "${OPTIONS[@]}"; do
        tmp="${CMD_BASE} ${option}"
        CMDs+=("$tmp")
    done
else
    CMDs=("$CMD_BASE")
fi

# number of times to submit
N=${#CMDs[@]}
# set seed; should only be done if N=1 (otherwise each job gets same seed variable)
if [ "$N" -eq "1" ]; then
    seed=""
fi

# Directory where this script is called
DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"

EXP_DIR="$(dirname "$DIR")" # get parent directory from where this script is called (assumes submitted from exp_dir/hpc_scripts)

# set base directory names to store results from jobs (specific directory will be given in the job script)
if [ ! -z ${SCRATCH} ]; then

    BASEDIR="${SCRATCH}/${PROJECT_DIR}"

    # create hpc_output directory if it does not exist
    if [ ! -d "${BASEDIR}/${EXP_NAME}/hpc_output" ]; then
        mkdir -p "${BASEDIR}/${EXP_NAME}/hpc_output"
    fi

    if [ ! -d "${BASEDIR}/${EXP_NAME}/results" ]; then
        mkdir -p "${BASEDIR}/${EXP_NAME}/results"
    fi

    VARIABLES="CODE_DIR=${EXP_DIR},WORKDIR_MOUNT=${WORKDIR_MOUNT},\
CONTAINER=${CONTAINER_NAME},RESULTS_MOUNT=${RESULTS_MOUNT},\
STUFF_TO_TMP=${STUFF_TO_TMP},RESULTS_TO_SCRATCH=${RESULTS_TO_SCRATCH},\
BASEDIR=${BASEDIR}"

    # SUBMIT JOBS
    for CMD_BASE_N in "${CMDs[@]}"
    do
        CMD_N="${CMD_BASE_N}"

        sbatch_cmd="--job-name=${EXP_NAME} \
                   --time=${TIME} \
                   --cpus-per-task=${CPUS_PER_TASK} \
                   --ntasks-per-core=1 \
                   --ntasks=1 \
                   --mem=${MEM} \
                   --chdir=${BASEDIR} \
                   -o ${BASEDIR}/${EXP_NAME}/hpc_output/${EXP_NAME}_%j.out \
                   --export=ALL,CMD=\"${CMD_N}\",\"${VARIABLES}\",EXP_NAME=${EXP_NAME}"

        re='^[0-9]+$'
        if  [[ ! $GPUS =~ $re ]] ; then
            echo "error: GPUS must be interger" >&2; exit 1
        else
            if [[ "$CLUSTER" = "PLAI" ]]; then
                if [[ "$GPUS" != "0" ]]; then
                    sbatch_cmd="${sbatch_cmd} --gres=gpu:${GPUS}"
                fi
                sbatch_cmd="${sbatch_cmd} --partition=${PARTITION}"
                bash -c "sbatch ${sbatch_cmd[@]} ~/hpc/plai_singularity_job.sh"
            elif [[ "$CLUSTER" = "CC" ]]; then
                if [[ "$GPUS" != "0" ]]; then
                    sbatch_cmd="${sbatch_cmd} --gres=gpu:${GPUS}"
                    #sbatch_cmd="${sbatch_cmd} --gres=gpu:${GPUS}"
                fi
                sbatch_cmd="${sbatch_cmd} --account=${ACCOUNT}"
                bash -c "sbatch ${sbatch_cmd[@]} ~/hpc/cc_singularity_job_single_task.sh"
            fi
            sleep 1 # sleep so that we do not overwhelm the scheduler (SEE e.g. compute canada's website)
        fi
    done
else
    echo "error while submitting job" >&2; exit 1
fi
